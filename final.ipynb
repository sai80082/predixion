{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45ddb51-719b-456e-8175-5de5d08d8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b65a4f-1d4a-465f-bf52-04084f50bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample conversation\n",
    "conversation = \"\"\"\n",
    "RA: नमस्ते श्री कुमार, मैं एक्स वाई जेड फाइनेंस से बोल रहा हूं । आपके लोन के बारे में बात करनी थी ।\n",
    "RA: सर, आपका पिछले महीने का EMI अभी तक नहीं आया है । क्या कोई समस्या है?\n",
    "RA: ओह , यह तो बरुा हुआ। लेकिन सर , आपको समझना होगा कि लोन का भगुतान समय पर करना बहुत जरूरी है ।\n",
    "RA: हम समझते हैं आपकी स्थिति । क्या आप अगले हफ्ते तक कुछ भगुतान कर सकते है ?\n",
    "RA: ठीक है , आधा भगुतान अगले हफ्ते तक कर दीजिए । बाकी का क्या प्लान है आपका ?\n",
    "RA: ठीक है । तो हम ऐसा करते हैं आप अगले हफ्ते तक आधा EMI जमा कर दीजिए, और अगले महीने के 15 तारीख तक बाकी का भगुतान कर दीजि ए। क्या यह आपको स्वीकार है ?\n",
    "RA: बहुत अच्छा । मैं आपको एक SMS भेज रहा हूं जिसमें भगुतान की डिटेल्स होंगी । कृपया इसका पालन करें और समय पर भगुतान करें ।\n",
    "RA: आपका स्वागत है । अगर कोई और सवाल हो तो मुझे बताइएगा। अलविदा ।\n",
    "B: हां, बोलिए। क्या बात है?\n",
    "B: हां, थोड़ी दिक्कत है। मेरी नौकरी चली गई है और मैं नया काम ढूंढ रहा हूं।\n",
    "B: मैं समझता हूं, लेकिन अभी मेरे पसै पैसे नहीं हैं। क्या कुछ समय मिल सकता है?\n",
    "B: मैं कोशिश करूंगा, लेकिन परूा EMI नहीं दे पाऊंगा। क्या आधा भगुतान चलेगा?\n",
    "B: मझु उम्मीद है कि अगले महीने तक मझु नया काम मिल जाएगा। तब मैं बाकी बकाया चकु दंगू।\n",
    "B: हां, यह ठीक रहेगा। मैं इस प्लान का पालन करनेकी परूी कोशि  करूंगा।\n",
    "B: ठीक है, धन्यवाद आपके समझने के लिए।\n",
    "B: अलविदा।\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df58ae45-a6fb-44a4-9803-fac5188ab46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "additional_stops = {\n",
    "    \"मैं\", \"हम\", \"आप\", \"यह\", \"वह\", \"मुझु\", \"मेरी\", \"आपके\", \n",
    "    \"है\", \"हूँ\", \"था\", \"हैं\", \"होगी\", \"कर सकते\", \n",
    "    \"और\", \"लेकिन\", \"या\", \"तो\", \"कि\", \n",
    "    \"में\", \"पर\", \"तक\", \"का\", \"की\", \"से\", \"के\", \n",
    "    \"यदि\", \"फिर\", \"जब\", \"ही\", \"जो\", \"अब\", \"क्या\",\"हां\",\"हूं\",\n",
    "    \",\", \"।\", \"?\"\n",
    "}\n",
    "\n",
    "categories = {\n",
    "    'payment': {'EMI', 'भुगतान', 'पैसे', 'रुपये', 'जमा'},\n",
    "    'deadline': {'तारीख', 'समय', 'दिन', 'महीने', 'हफ्ते'},\n",
    "    'problem': {'समस्या', 'दिक्कत', 'परेशानी', 'मुश्किल'},\n",
    "    'agreement': {'ठीक', 'सहमत', 'स्वीकार', 'मंजूर'}\n",
    "}\n",
    "sentiment_lexicon = {\n",
    "    'positive': {\n",
    "        'ठीक', 'अच्छा', 'बढ़िया', 'सहमत', 'धन्यवाद', 'स्वीकार',\n",
    "        'मदद', 'कृपया', 'शुक्रिया'\n",
    "    },\n",
    "    'negative': {\n",
    "        'समस्या', 'दिक्कत', 'परेशानी', 'मुश्किल', 'नहीं', 'बकाया',\n",
    "        'देरी', 'गलत', 'माफ़'\n",
    "    },\n",
    "    'neutral': {\n",
    "        'है', 'हूं', 'था', 'थी', 'और', 'या', 'में', 'पर'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a3dc8d-d398-4661-b111-8ef84a0c9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize existing components\n",
    "factory = IndicNormalizerFactory()\n",
    "normalizer = factory.get_normalizer(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31e5eda-6ccd-4c49-acfa-951f1d4b8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer( text: str) -> List[str]:\n",
    "    normalized_text = normalizer.normalize(text)\n",
    "    tokens = indic_tokenize.trivial_tokenize(normalized_text)\n",
    "    return [\n",
    "        word for word in tokens \n",
    "        if word not in additional_stops\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ca83bb-d23f-42de-8b9f-307992e3f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0a3d20-38c1-496a-9bf1-a87797d323ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text( text):\n",
    "    normalized_text = normalizer.normalize(text)\n",
    "    tokens = custom_tokenizer(normalized_text)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa268d05-13a0-458e-b9e3-137cc2540fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogues( text):\n",
    "    dialogues = []\n",
    "    lines = text.strip().split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('RA:'):\n",
    "            speaker = 'Recovery_Agent'\n",
    "            content = line[3:].strip()\n",
    "        elif line.startswith('B:'):\n",
    "            speaker = 'Borrower'\n",
    "            content = line[2:].strip()\n",
    "        else:\n",
    "            continue\n",
    "        processed_content = preprocess_text(content)\n",
    "        \n",
    "        dialogues.append({\n",
    "            'speaker': speaker,\n",
    "            'original_content': content,\n",
    "            'processed_content': processed_content\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad3e5eaa-5090-4adc-8f9c-70acce8f9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze sentiment of Hindi text.\"\"\"\n",
    "    tokens = custom_tokenizer(text)\n",
    "    \n",
    "    # Count sentiment words\n",
    "    sentiment_counts = {\n",
    "        'positive': sum(1 for token in tokens if token in sentiment_lexicon['positive']),\n",
    "        'negative': sum(1 for token in tokens if token in sentiment_lexicon['negative']),\n",
    "        'neutral': sum(1 for token in tokens if token in sentiment_lexicon['neutral'])\n",
    "    }\n",
    "    \n",
    "    total_sentiment_words = sum(sentiment_counts.values())\n",
    "    if total_sentiment_words == 0:\n",
    "        sentiment_score = 0\n",
    "    else:\n",
    "        sentiment_score = (sentiment_counts['positive'] - sentiment_counts['negative']) / total_sentiment_words\n",
    "    \n",
    "    # Determine sentiment label\n",
    "    if sentiment_score > 0.1:\n",
    "        sentiment = 'positive'\n",
    "    elif sentiment_score < -0.1:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "        \n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'score': sentiment_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac59363b-7a03-4b12-a505-47a13d581c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(dialogue_df: pd.DataFrame):\n",
    "    total_turns = len(dialogue_df)\n",
    "    speaker_counts = dialogue_df['speaker'].value_counts()\n",
    "    \n",
    "    _, importance_scores = analyze_with_tfidf(dialogue_df['processed_content'])\n",
    "    top_topics = importance_scores.head(3)['term'].tolist()\n",
    "    \n",
    "    sentiment_by_speaker = {}\n",
    "    for speaker in dialogue_df['speaker'].unique():\n",
    "        speaker_content = dialogue_df[dialogue_df['speaker'] == speaker]['original_content']\n",
    "        speaker_sentiments = [analyze_sentiment(content)['score'] for content in speaker_content]\n",
    "        avg_sentiment = sum(speaker_sentiments) / len(speaker_sentiments)\n",
    "        \n",
    "        if avg_sentiment > 0.2:\n",
    "            sentiment_by_speaker[speaker] = \"Positive\"\n",
    "        elif avg_sentiment < -0.2:\n",
    "            sentiment_by_speaker[speaker] = \"Negative\"\n",
    "        else:\n",
    "            sentiment_by_speaker[speaker] = \"Neutral\"\n",
    "\n",
    "    summary_points = []\n",
    "    for idx, row in dialogue_df.iterrows():\n",
    "        sentiment = analyze_sentiment(row['original_content'])\n",
    "        if sentiment['score'] > 0.2 or sentiment['score'] < -0.2:\n",
    "            summary_points.append({\n",
    "                'speaker': row['speaker'],\n",
    "                'content': row['original_content'],\n",
    "                'sentiment': sentiment['sentiment']\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'statistics': {\n",
    "            'total_turns': total_turns,\n",
    "            'speaker_distribution': speaker_counts.to_dict(),\n",
    "            'main_topics': top_topics\n",
    "        },\n",
    "        'sentiment_by_speaker': sentiment_by_speaker,\n",
    "        'key_points': summary_points[:5]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2fd26a-19c0-434f-83f4-acb2361459fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_tfidf( texts):\n",
    "    tfidf_matrix = tfidf.fit_transform(texts)\n",
    "    \n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(\n",
    "        tfidf_matrix.toarray(),\n",
    "        columns=feature_names\n",
    "    )\n",
    "    importance_scores = pd.DataFrame({\n",
    "        'term': feature_names,\n",
    "        'score': tfidf_matrix.sum(axis=0).A1\n",
    "    })\n",
    "    importance_scores = importance_scores.sort_values('score', ascending=False)\n",
    "    return tfidf_df, importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b898c4-c297-4064-bc5f-0d7bd744ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_terms( importance_scores):\n",
    "    categorized_terms = defaultdict(list)\n",
    "    \n",
    "    for _, row in importance_scores.iterrows():\n",
    "        term = row['term']\n",
    "        score = row['score']\n",
    "    \n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in term for keyword in keywords):\n",
    "                categorized_terms[category].append({\n",
    "                    'term': term,\n",
    "                    'score': score\n",
    "                })\n",
    "                break\n",
    "        \n",
    "    return categorized_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e3121a-c448-4d2c-92f1-5272c2b482fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_phrases( importance_scores, threshold=0.1):\n",
    "    key_phrases = []\n",
    "    \n",
    "    for _, row in importance_scores.iterrows():\n",
    "        if row['score'] > threshold:\n",
    "            key_phrases.append({\n",
    "                'phrase': row['term'],\n",
    "                'importance': row['score']\n",
    "            })\n",
    "    return key_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eb8bbe8-f35a-4377-906d-73dd564cec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_conversation( conversation_text: str) -> Dict[str, Any]:\n",
    "    df = extract_dialogues(conversation_text)\n",
    "    tfidf_df, importance_scores = analyze_with_tfidf(df['processed_content'])\n",
    "    categorized_terms = categorize_terms(importance_scores)\n",
    "    key_phrases = extract_key_phrases(importance_scores)\n",
    "    summary = generate_summary(df)\n",
    "    sentiment_analysis = []\n",
    "    for _, row in df.iterrows():\n",
    "        sentiment = analyze_sentiment(row['original_content'])\n",
    "        sentiment_analysis.append({\n",
    "            'speaker': row['speaker'],\n",
    "            'content': row['original_content'],\n",
    "            'sentiment': sentiment\n",
    "        })\n",
    "    return {\n",
    "        'dialogue_analysis': df.to_dict('records'),\n",
    "        'categorized_terms': categorized_terms,\n",
    "        'key_phrases': key_phrases,\n",
    "        'summary': summary,\n",
    "        'sentiment_analysis': sentiment_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0396a4d5-64bd-4328-a0a7-51a85340e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/home/dev/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/home/dev/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = analyze_conversation(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75feb88-4c55-4ae7-b489-142be93b676a",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dc4ebaa-ac4e-49b3-b50f-6a05413ae189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation is a 16-turn dialogue between a recovery agent and a borrower. Recovery_Agent contributed to 50.00% of the conversation. Borrower contributed to 50.00% of the conversation. The primary focus was on resolving outstanding financial obligations. Key payment-related terms included: पसै पैसे, पैसे नहीं, पैसे. The conversation revealed significant financial challenges. Specific problem indicators: समस्या. The conversation's emotional landscape was characterized by 5 moments of tension or concern and 5 instances of constructive dialogue. Significant emphasis was placed on timing and scheduled resolutions. Key timing-related terms: अगले हफ्ते, हफ्ते, समय, महीने, अगले महीने, हफ्ते कुछ, समय मिल, पिछले महीने, समय करना, भगुतान समय, हफ्ते कर, समय भगुतान, हफ्ते आधा. The dialogue culminated in a mutually acceptable resolution strategy. Resolution indicators: ठीक, स्वीकार. Critical discussion anchors: अलविदा, भगुतान, ठीक. The interaction exemplifies a nuanced approach to financial negotiation, balancing institutional requirements with individual circumstances.\n"
     ]
    }
   ],
   "source": [
    "def generate_comprehensive_summary(results):\n",
    "    categorized_terms = results.get('categorized_terms', {})\n",
    "    key_phrases = results.get('key_phrases', [])\n",
    "    sentiment_analysis = results.get('sentiment_analysis', [])\n",
    "    summary_stats = results.get('summary', {}).get('statistics', {})\n",
    "    dialogue_analysis = results.get('dialogue_analysis', [])\n",
    "    summary_parts = []\n",
    "    total_turns = summary_stats.get('total_turns', 0)\n",
    "    speaker_distribution = summary_stats.get('speaker_distribution', {})\n",
    "    if total_turns > 0:\n",
    "        summary_parts.append(f\"The conversation is a {total_turns}-turn dialogue between a recovery agent and a borrower.\")\n",
    "        if speaker_distribution:\n",
    "            for speaker, turns in speaker_distribution.items():\n",
    "                participation_percentage = (turns / total_turns) * 100\n",
    "                summary_parts.append(f\"{speaker} contributed to {participation_percentage:.2f}% of the conversation.\")\n",
    "    \n",
    "    if 'payment' in categorized_terms:\n",
    "        payment_terms = categorized_terms['payment']\n",
    "        summary_parts.append(\"The primary focus was on resolving outstanding financial obligations.\")\n",
    "        \n",
    "        payment_phrases = [term['term'] for term in payment_terms]\n",
    "        if payment_phrases:\n",
    "            summary_parts.append(f\"Key payment-related terms included: {', '.join(payment_phrases)}.\")\n",
    "    \n",
    "    if 'problem' in categorized_terms:\n",
    "        problem_terms = categorized_terms['problem']\n",
    "        problem_phrases = [term['term'] for term in problem_terms]\n",
    "        summary_parts.append(\"The conversation revealed significant financial challenges.\")\n",
    "        \n",
    "        if problem_phrases:\n",
    "            summary_parts.append(f\"Specific problem indicators: {', '.join(problem_phrases)}.\")\n",
    "    \n",
    "    negative_sentiments = [turn for turn in sentiment_analysis if turn['sentiment']['sentiment'] == 'negative']\n",
    "    positive_sentiments = [turn for turn in sentiment_analysis if turn['sentiment']['sentiment'] == 'positive']\n",
    "    \n",
    "    if negative_sentiments or positive_sentiments:\n",
    "        sentiment_summary = \"The conversation's emotional landscape was characterized by \"\n",
    "        sentiment_components = []\n",
    "        \n",
    "        if negative_sentiments:\n",
    "            sentiment_components.append(f\"{len(negative_sentiments)} moments of tension or concern\")\n",
    "        \n",
    "        if positive_sentiments:\n",
    "            sentiment_components.append(f\"{len(positive_sentiments)} instances of constructive dialogue\")\n",
    "        \n",
    "        summary_parts.append(sentiment_summary + \" and \".join(sentiment_components) + \".\")\n",
    "    \n",
    "    if 'deadline' in categorized_terms:\n",
    "        deadline_terms = categorized_terms['deadline']\n",
    "        deadline_phrases = [term['term'] for term in deadline_terms]\n",
    "        summary_parts.append(\"Significant emphasis was placed on timing and scheduled resolutions.\")\n",
    "        \n",
    "        if deadline_phrases:\n",
    "            summary_parts.append(f\"Key timing-related terms: {', '.join(deadline_phrases)}.\")\n",
    "    \n",
    "    if 'agreement' in categorized_terms:\n",
    "        agreement_terms = categorized_terms['agreement']\n",
    "        agreement_phrases = [term['term'] for term in agreement_terms]\n",
    "        summary_parts.append(\"The dialogue culminated in a mutually acceptable resolution strategy.\")\n",
    "        \n",
    "        if agreement_phrases:\n",
    "            summary_parts.append(f\"Resolution indicators: {', '.join(agreement_phrases)}.\")\n",
    "    \n",
    "    # Key Phrases Interpretation\n",
    "    if key_phrases:\n",
    "        top_phrases = [phrase['phrase'] for phrase in key_phrases[:3]]\n",
    "        summary_parts.append(f\"Critical discussion anchors: {', '.join(top_phrases)}.\")\n",
    "    \n",
    "    # Concluding Insight\n",
    "    final_summary = \" \".join(summary_parts)\n",
    "    final_summary += \" The interaction exemplifies a nuanced approach to financial negotiation, balancing institutional requirements with individual circumstances.\"\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "comprehensive_summary = generate_comprehensive_summary(results)\n",
    "print(comprehensive_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ac30d-1722-4e2b-a4d8-ded14a9cb0db",
   "metadata": {},
   "source": [
    "## Key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77682375-5531-4806-8aa8-f4913f987552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key Points:\n",
      "- Recovery_Agent: सर, आपका पिछले महीने का EMI अभी तक नहीं आया है । क्या कोई समस्या है?\n",
      "- Recovery_Agent: ठीक है , आधा भगुतान अगले हफ्ते तक कर दीजिए । बाकी का क्या प्लान है आपका ?\n",
      "- Recovery_Agent: ठीक है । तो हम ऐसा करते हैं आप अगले हफ्ते तक आधा EMI जमा कर दीजिए, और अगले महीने के 15 तारीख तक बाकी का भगुतान कर दीजि ए। क्या यह आपको स्वीकार है ?\n",
      "- Recovery_Agent: बहुत अच्छा । मैं आपको एक SMS भेज रहा हूं जिसमें भगुतान की डिटेल्स होंगी । कृपया इसका पालन करें और समय पर भगुतान करें ।\n",
      "- Borrower: हां, थोड़ी दिक्कत है। मेरी नौकरी चली गई है और मैं नया काम ढूंढ रहा हूं।\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nKey Points:\")\n",
    "for point in results['summary']['key_points']:\n",
    "    print(f\"- {point['speaker']}: {point['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4c369-d5f7-4359-a411-e8334c60eeb3",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1b65561-08d4-48c3-adf1-2fd0bfd245d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Analysis:\n",
      "Recovery_Agent: नमस्ते श्री कुमार, मैं एक्स वाई जेड फाइनेंस से बोल रहा हूं । आपके लोन के बारे में बात करनी थी । -> neutral\n",
      "Recovery_Agent: सर, आपका पिछले महीने का EMI अभी तक नहीं आया है । क्या कोई समस्या है? -> negative\n",
      "Recovery_Agent: ओह , यह तो बरुा हुआ। लेकिन सर , आपको समझना होगा कि लोन का भगुतान समय पर करना बहुत जरूरी है । -> neutral\n",
      "Recovery_Agent: हम समझते हैं आपकी स्थिति । क्या आप अगले हफ्ते तक कुछ भगुतान कर सकते है ? -> neutral\n",
      "Recovery_Agent: ठीक है , आधा भगुतान अगले हफ्ते तक कर दीजिए । बाकी का क्या प्लान है आपका ? -> positive\n",
      "Recovery_Agent: ठीक है । तो हम ऐसा करते हैं आप अगले हफ्ते तक आधा EMI जमा कर दीजिए, और अगले महीने के 15 तारीख तक बाकी का भगुतान कर दीजि ए। क्या यह आपको स्वीकार है ? -> positive\n",
      "Recovery_Agent: बहुत अच्छा । मैं आपको एक SMS भेज रहा हूं जिसमें भगुतान की डिटेल्स होंगी । कृपया इसका पालन करें और समय पर भगुतान करें । -> positive\n",
      "Recovery_Agent: आपका स्वागत है । अगर कोई और सवाल हो तो मुझे बताइएगा। अलविदा । -> neutral\n",
      "Borrower: हां, बोलिए। क्या बात है? -> neutral\n",
      "Borrower: हां, थोड़ी दिक्कत है। मेरी नौकरी चली गई है और मैं नया काम ढूंढ रहा हूं। -> negative\n",
      "Borrower: मैं समझता हूं, लेकिन अभी मेरे पसै पैसे नहीं हैं। क्या कुछ समय मिल सकता है? -> negative\n",
      "Borrower: मैं कोशिश करूंगा, लेकिन परूा EMI नहीं दे पाऊंगा। क्या आधा भगुतान चलेगा? -> negative\n",
      "Borrower: मझु उम्मीद है कि अगले महीने तक मझु नया काम मिल जाएगा। तब मैं बाकी बकाया चकु दंगू। -> negative\n",
      "Borrower: हां, यह ठीक रहेगा। मैं इस प्लान का पालन करनेकी परूी कोशि  करूंगा। -> positive\n",
      "Borrower: ठीक है, धन्यवाद आपके समझने के लिए। -> positive\n",
      "Borrower: अलविदा। -> neutral\n",
      "Overall sentiment: {'Recovery_Agent': 'Positive', 'Borrower': 'Negative'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSentiment Analysis:\")\n",
    "for turn in results['sentiment_analysis']:\n",
    "    print(f\"{turn['speaker']}: {turn['content']} -> {turn['sentiment']['sentiment']}\")\n",
    "print(f\"Overall sentiment: {results['summary']['sentiment_by_speaker']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6ca97-5759-478e-8a23-1dd5fe5acb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
